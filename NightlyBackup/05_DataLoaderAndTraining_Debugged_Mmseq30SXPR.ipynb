{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training\n",
    "In this notebook, we will show\n",
    "\n",
    "* How to construct a training and validation dataset that respect External Symmetry. Disconnection on the BC Clan graph will satisfy fairness in External Symmetry; this forms a testing dataset ready for k-fold cross validation.\n",
    "* How to train some models in a 3-fold CV scheme. The training will be done with pytorch taking advantage of its dataloader.\n",
    "* Several effective data augmentation strategies popularised in residual network training.\n",
    "\n",
    "We will illustrate this with training on classification of the base/nonsite/phosphate/ribose `S,X,P,R` dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# ============== Click Please.Imports\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import io\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "import collections\n",
    "\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import torchvision as tv\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from NucleicNet.DatasetBuilding.util import *\n",
    "#from NucleicNet.DatasetBuilding.commandReadPdbFtp import ReadBCExternalSymmetry, MakeBcClanGraph\n",
    "from NucleicNet.DatasetBuilding.commandDataFetcherMmseq import FetchIndex, FetchTask, FetchDataset\n",
    "from NucleicNet import Burn, Fuel\n",
    "import NucleicNet.Burn.util\n",
    "import NucleicNet.Burn.M1\n",
    "import  NucleicNet.Burn.DA\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "\n",
    "\n",
    "# Turn on cuda optimizer\n",
    "print(torch.backends.cudnn.is_available())\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# disable debugs NOTE use only after debugging\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "# Disable gradient tracking\n",
    "#torch.no_grad()\n",
    "#torch.inference_mode()\n",
    "\n",
    "# ================= Click Please. Directories ==================\n",
    "DIR_DerivedData = \"../Database-PDB/DerivedData/\"\n",
    "DIR_Typi = \"../Database-PDB/typi/\"\n",
    "DIR_FuelInput = \"../Database-PDB/feature/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scope of Data\n",
    "\n",
    "The cell below defines the scope of data to be used in training SXPR classifcation. We have updated the curation to year 2021 but we cannot guarentee the curation using flags below will suffice the need of our community thereafter without an update. As in this classification task, we need not care about base specificity (just the presence of the base is needed!), there are much more entries we can include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731, 34)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO For SXPR the selection of pdb has to be separated less stringent than AUCG\n",
    "\n",
    "Df_grand = pd.read_pickle(DIR_DerivedData + \"/DataframeGrand.pkl\")\n",
    "Df_grand = Df_grand.loc[(Df_grand[\"ProNu\"] == \"prot-nuc\") & (Df_grand['Resolution'] <= 3.5) # NOTE you may consider to relax the 3.0 Angstron resolution limit as cryoEM structure w/ ~3.5 angstrom are not uncommon to be modelled in full atom these days\n",
    "                                          & ~(Df_grand['PubmedID'].isnull()) # NOTE ~78 structures. Note that some are recent unindexed by pdb; most are unpublished structures. Some contains large missing loops.\n",
    "                                          & (Df_grand['NucleicAcid'].isin(['rna']))\n",
    "                                          & (pd.notnull(Df_grand['InternalSymmetryBC-95']))\n",
    "                                          & (Df_grand[\"Year\"] <= 2021)\n",
    "                                          & (Df_grand[\"MeanChainLength_Nucleotide\"] >= 4) & (Df_grand[\"SumChainLength_Peptide\"] > 50) \n",
    "                                          # NOTE Some machineries that do not show preference in base or a disproportionately small amount of sites with preference.\n",
    "                                          & ~(Df_grand[\"Title\"].str.contains('ribos|trna', regex=True, na=False)) \n",
    "                                          & ~(Df_grand[\"Header\"].str.contains('ribos|trna', regex=True, na=False))\n",
    "#                                          & ~(Df_grand[\"Title\"].str.contains('ribos|riboz|transcript|polymerase|trna|pseudouridine|srp|signal recognition particle| ribonuclease|exosome|spliceosome', regex=True, na=False)) \n",
    "#                                          & ~(Df_grand[\"Header\"].str.contains('ribos|riboz|transcript|polymerase|trna|pseudouridine|srp|signal recognition particle| ribonuclease|exosome|spliceosome', regex=True, na=False))\n",
    "                                          & ~(Df_grand['NpidbClassification'].isin([\"TRANSFERASE/RNA\",'TRANSFERASE','RIBOSOME'])) # ~170 structures\n",
    "\n",
    "                                          # NOTE Unpublished but with pubmedid?\n",
    "                                          & ~(Df_grand['Pdbid'].isin(['3p6y', '2n8m', '3ahu', '3boy']))\n",
    "                                          # NOTE Cases where metal/interfacial inhibitor/tip of hairpin/water-mediated/marginally/modified base interacts with rna base\n",
    "                                          & ~(Df_grand['Pdbid'].isin([  \n",
    "                                                                      \n",
    "                                                                        '4oq8', '1a34','2bbv','1ddl','4oq9','4nia','4ang','2b2e',# NOTE Virus capsid with overlappping atoms(?)\n",
    "                                                                        '2bny','2b2g','1aq4','1zse','2iz8','2bq5','2bs1','2izm','2izn',\n",
    "                                                                        '2c51','2c50', '2izm', '2izn', '6msf', '7msf', '1bmv','1zdh','1zdi', '1zdj', '1zdk','1aq3',# NOTE Assembled virus capsid can sometime miss half of binding sites (e.g. 2c51 3 equiv protein chain but due to overlaps in assebly only one rna chain appear in some copies) \n",
    "                                                                        '4oav', # NOTE Modified nucleotide backbone \n",
    "                                                                         '7njc', # NOTE Modified nucleotide backbone\n",
    "                                                                        '4eya', '4ghl','4ghl', '6o5f', '6sx2',\n",
    "                                                                        '6sx0','6zlc', '2zko',# NOTE water envelop\n",
    "                                                                        '1m8v', # NOTE very low clashscore... somethings wrong?\n",
    "\n",
    "\n",
    "                                                                        \n",
    "                                                                        '5zc9', # NOTE eIF4A1 chemical clamp, water\n",
    "                                                                        '6xki', # NOTE eIF4A1 chemical clamp, water\n",
    "                                                                        '6r7g', # NOTE only 6 units assembled at interface\n",
    "                                                                        #'4bkk', # NOTE nucleoprotein. There is no mention of base interaction through out the article https://www.microbiologyresearch.org/content/journal/jgv/10.1099/vir.0.053025-0\n",
    "                                                                        #'6yrb','6yrq', # NOTE No base interaction mentioned in paper (Check again)\n",
    "                                                                        #'1yyw', '2nug', '2nue', '1yz9',# NOTE These is a AU dsRna but prefer GU in other Rnase3 at Q157, 1yz9 makes no contact w/ base\n",
    "                                                                        #'2bs0', # NOTE RNA at interface of two varial capsid protein symmetry mates\n",
    "                                                                        #'7n0c','7n0b','7n0d', # NOTE exoribonuclease proof-reading complex but when mismatch the base makes no touch\n",
    "                                                                        #'2xgj', # NOTE Helicase w/ no touch at base\n",
    "\n",
    "\n",
    "\n",
    "                                                                        # NOTE Structure solved with Poly-Oligonucleotiude just as a template\n",
    "                                                                        #'5wwf', '5ho4', # NOTE These are proteins resolved with same interacting sequence. Its siblings 5wwg 5wwe 5wwx makes most contact with the protein\n",
    "                                                                        '4ht8', '3gib', # NOTE 4ht9 has a higher resolution also with additional uridine sites shown\n",
    "                                                                        #'4ijs', # NOTE They use a polyA sequence for simplicity. even though there are interaction with some of the bases.\n",
    "                                                                        '2xbm', # NOTE specificity is in a dinucleotide labeled as G3A\n",
    "                                                                        \n",
    "                                                                        '5eeu', '5eev', '5eew', '5eex', '5eey', '5eez', \n",
    "                                                                        '5ef0', '5ef1', '5ef2', '5ef3', '1utd', '4v4f', \n",
    "                                                                        '1gtf', '1gtn', # NOTE While the protein is the same, RNA does not show up in a pseudo symmetry mate. Half and Half. also note a lot of unmodeled nt https://www.rcsb.org/3d-view/5EEV/1\n",
    "                                                                        #'6dtd', #  NOTE Cas 13b\n",
    "                                                                        '2zi0', '4erd', # NOTE single helix contact\n",
    "                                                                        '6cf2', # NOTE single helix contact\n",
    "\n",
    "                                                                        #'6mdz', # TODO Ttesting\n",
    "                                                                        #'5js2', '5ki6', # NOTE Modified base argonaut\n",
    "                                                                        #'6oon','5vm9','5w6v','4kre','4kxt','4olb','4ola', # NOTE Poly-A sequence bound to argonaut\n",
    "                                                                        #'5t7b' # NOTE unpublished argonaut\n",
    "                                                                        #'4z4c', '4z4d', '4z4e', '4z4f', '4z4g', '4z4h', '4z4i', # NOTE This series of pdbid concerns a water mediated recognition site for adenosine on argonaute `Water-mediated recognition of t1-adenosine anchors Argonaute2 to microRNA targets`\n",
    "                                                                        #'5js1', '4w5o', '4w5q', # NOTE Argonaut structure. 4w5o,q has more missing residue than siblings 4w5t,r,n.\n",
    "                                                                        #'5wqe', # NOTE multiplebase specific interactions were outlined but most interacts with peptide backbone.\n",
    "                                                                        #'5wtk', # NOTE 4 base specific interactions were outlined but the structure is ds and some sidechains e.g. 415-416 were stubbed. we will not include it in training\n",
    "\n",
    "                                                                        # NOTE No specific H bond contact found/does not fulfill Hbond criterion in pymol\n",
    "                                                                        #'5ztm', # NOTE The claimed interaction at E172, N175, Q195 does not fulfill H-bond criterion in pymol. Find>Polar Contacts\n",
    "                                                                        #'6h5s','6h5q', # NOTE no specific H bond  contact\n",
    "                                                                        #'4al7','4al5','4al6', # NOTE base binding site at an unmodeled loop\n",
    "                                                                        #'4n2s','4n2q','4me2', # NOTE close but no defined H bond \n",
    "                                                                        #'6hyu', '6hyt', # NOTE polyA used and no Hbond specific contact\n",
    "                                                                        \n",
    "\n",
    "\n",
    "                                                                        #'5t8y', \n",
    "\n",
    "                                                                        #'4z92', # NOTE minimal contact in vriys \n",
    "                                                                        #'3hsb', # NOTE a AGAGAG aptamer used but the G does not form specific hbond interactions \n",
    "                                                                        #'7bg6','7bg7','7nuq','7nun','7nuo','7nul','7num', # NOTE only stack touched\n",
    "                                                                        #'5f9f', '5f98','5f9h','5e3h','3eqt', # NOTE RIG-I recognise modified base m7G `https://www.pnas.org/doi/full/10.1073/pnas.1515152113`\n",
    "                                                                        #'5z98','4lg2', # NOTE duplex\n",
    "                                                                        '2ihx', # NOTE Disordered\n",
    "                                                                        #'4gv3','4gv6','4gv9','4gve','4g9z', #NOTE backbone only\n",
    "                                                                        #'7c06', # NOTE it shares same sequence ith 7c08 but poor?\n",
    "                                                                        \n",
    "                                                                        #'3ciy' # NOTE dsRNA\n",
    "                                                                        #'5jbg', # NOTE MDA5\n",
    "                                                                        '4ill', '4ilm', '4ilr', # NOTE The RNA strand appears broken??? (bonds too long)\n",
    "                                                                        #'6s8b','6s8e','6shb','6sic','6s91','6s6b', # NOTE Backbone only. marginal interaction\n",
    "\n",
    "                                                                        '4peh','4peg','4pei','4pef', # NOTE modified base\n",
    "\n",
    "                                                                        #'5jaj','5jb2','5jbg', # NOTE LGP2 duplex\n",
    "                                                                        #'4lg2', # NOTE duplex\n",
    "                                                                        #'4gha','5m73', # NOTE dsrna\n",
    "\n",
    "                                                                        '3ciy', # NOTE 3.41 angstrom resolution, some sidechain can be highly flexible\n",
    "                                                                        \n",
    "                                                                        \n",
    "\n",
    "                                                                        #'3zd6','3zd7', # NOTE Rig I\n",
    "\n",
    "                                                                        '3zc0', # NOTE almost no contact\n",
    "                                                                        '2jlw', # NOTE no contact\n",
    "                                                                        #'6ozp', '6ozn', '6ozf','6oze', '6ozg', '6ozh','6ozi', '6ozj', '6ozk', '6ozl', '6ozm',  '6ozo',  '6ozq', \n",
    "                                                                        #'6ozr','6ozs', # NOTE through backbone\n",
    "                                                                        #'2gje', # NOTE backbone only\n",
    "                                                                        #'1f8v', '2bbv', # NOTE backbone only duplex cage in virus capsid\n",
    "\n",
    "                                                                        \n",
    "                                                                        '2mxy', # NOTE solution structure with extra nucleotide compare to 2mz1\n",
    "                                                                       \n",
    "\n",
    "                                                                        '3pkm', # NOTE missing loop\n",
    "\n",
    "                                                                        \n",
    "                                                                        \n",
    "                                                                        '2bx2', # NOTE Marginal\n",
    "\n",
    "                                                                        #'6d06', # NOTE modified base dsrna\n",
    "                                                                        '3dh3', # NOTE Modified base\n",
    "                                                                        '7kfn', # NOTE Modified base\n",
    "                                                                        '4i67', # NOTE Modified nt\n",
    "                                                                        '1jbt','1jbs','1jbr',\n",
    "                                                                        '6gc5', #NOTE short strand\n",
    "                                                                        \n",
    "\n",
    "                                                                        \n",
    "                                                                        '5uj2', # NOTE marginal; same family as 4e78\n",
    "\n",
    "                                                                        '7ndh', '7ndi', '7ndj', '7ndk','3d2s','3trz', # NOTE require zinc cage\n",
    "                                                                        '6l1w', '1rgo', # NOTE zinc finger    \n",
    "                                                                        '4lj0', '5elk',# NOTE Zn finger short peptide\n",
    "                                                                        '2hgh','2li8','6wlh', # NOTE Zn finger short peptide\n",
    "\n",
    "                                                                        '2mqv','2mqt','2ms0','2ms1','2mkn','5u9b','1wwe','1wwf','1wwd','1wwg','2n82','5u9b','1fje','1t4l',\n",
    "                                                                        '2l3c','2lup','1a1t','2mf1','2mf0','1f6u','1ekz','6gbm','2mfe', \n",
    "                                                                        '2mfg', '2mfh','2mff', '4cio','2jpp',#NOTE Disordered NMR solution structures\n",
    "                                                                        \n",
    "                                                                        '5c0y','5v7c', # NOTE no contact\n",
    "                                                                        #'5wea', # NOTE poly A sequence\n",
    "\n",
    "                                                                        #'6vff', # NOTE dsrna\n",
    "                                                                        #'7krn', '7kro','7krp', # NOTE Helicase dsrna\n",
    "                                                                        '4pmi', # NOTE single helix\n",
    "                                                                        '6yrb', # NOTE The nucleotide is detached?\n",
    "                                                                        '2vpl', # NOTE require potassium coordination\n",
    "\n",
    "                                                                        # NOTE Water-mediated or simply in an envelope of water\n",
    "                                                                        '1wpu','2qux','1wmq','1wrq','4csf',\n",
    "                                                                        '4qoz', '4tuw','4tux','4tv0','4l8r', # NOTE water duplex\n",
    "                                                                        '4mdx', # NOTE water\n",
    "                                                                        '5l2l', # NOTE water\n",
    "                                                                        '5elh', # NOTE water; 5elk has much tighter contact \n",
    "                                                                        '2pjp', '6lt7','6db8','6db9','1c9s','6c6k', '3ts2','5tf6',\n",
    "                                                                        '4n0t','4kzd','6b3k','5e08','5h1l', '1m5o', '6fq3',\n",
    "                                                                        '5gxh','4q9q', '6mwn','5det','6u8d','6u8k', '5gxi','6hau','6d12',\n",
    "                                                                        '2y8y','2y9h','2y8w','4qvc','4f02','6fql','6fq3', '4ht9', # NOTE water\n",
    "                                                                        ]))\n",
    "                                          # NOTE Recently indexed shape-dependent machinery (tRNA/exosome/ribosome), but pdb has not updated its derived data\n",
    "                                          & ~(Df_grand['Pdbid'].isin(['5hr7','5omw','5jea',\n",
    "                                                                      #'4o26', # NOTE telomerase\n",
    "                                                                      #'5fmz','5epi', # NOTE polymerase\n",
    "                                                                      '6zoj', '6zok', '6zol', # NOTE Ribosome\n",
    "                                                                      '6yan','6yam','6yal', # NOTE ribosome\n",
    "                                                                      '5iwa', # NOTE ribosome\n",
    "                                                                      '5e6m', # NOTE trna\n",
    "                                                                      '5on2','5onh','5on3','5omw','3al0', '3akz', '5e6m', # NOTE tRNA \n",
    "                                                                      '1zl3', # NOTE trna specificity at modified base FLO\n",
    "                                                                      '5ud5','5v6x','4qei','4kqe' # NOTE trna\n",
    "                                                                      '3jam','3jap','3jaq', # NOTE This is a ribosome\n",
    "                                                                      #'5ng6', # NOTE Crispr machinery recognise DNA motif TTN but no mention of RNA\n",
    "                                                                      #'6sh8','6s6b', '6s8b', '6s8e', '6s91', '6shb', '6sic', # NOTE Crispr machinery no mention of base interaction\n",
    "                                                                    ]))\n",
    "\n",
    "\n",
    "                                          # NOTE \n",
    "                                              ]\n",
    "#print(pd.unique(Df_grand['NucleicAcid']))\n",
    "print(Df_grand.shape)\n",
    "# NOTE Further Remarks on some interesting cases\n",
    "# 3PTO, 3PTX, 3PU0, 3PU1, 3PU4. uses the same nucleocapsid to bind with poly(A,U,C,G), which they use to test how interaction with each kind of base will look like and they propose UAG as an interesting motif to look for https://journals.asm.org/doi/10.1128/JVI.01927-10\n",
    "#                               polyG shows largest amount of interaction polyU shows none However at 3.0 Angstrom, the assignment of N161 can be flipped to make interaction with U27 (seem to support by K164)\n",
    "# 6O1K, 6O1L, 6O1M              `Hfq thus has a structural preference for (ARN)n RNA stretches on its distal side, where N is any nucleotide. `\n",
    "\n",
    "\n",
    "NmrStates = [ '1aud00000004','1aud00000010','1aud00000002',\n",
    "              '2l4100000005','2l4100000011','2l4100000013',\n",
    "              '2xc700000000','2xc700000002','2xc700000006',\n",
    "              '1dz500000007','1dz500000008','1dz500000002',\n",
    "              '1k1g00000001','1k1g00000005','1k1g00000007',\n",
    "              '2ad900000017','2ad900000012','2ad900000019',\n",
    "              '2adb00000004','2adb00000005','2adb00000014',\n",
    "              '2adc00000007','2adc00000001','2adc00000000',\n",
    "              '2c0600000002','2c0600000004','2c0600000009',\n",
    "              '2cjk00000007','2cjk00000008','2cjk00000012',\n",
    "              '2err00000003','2err00000016','2err00000006',\n",
    "              '2fy100000008','2fy100000002','2fy100000000',\n",
    "              '2kfy00000006','2kfy00000003','2kfy00000001',\n",
    "              '2kg000000019','2kg000000012','2kg000000000',\n",
    "              '2kg100000006','2kg100000005','2kg100000003',\n",
    "              '2kh900000007','2kh900000001','2kh900000005',\n",
    "              '2km800000004','2km800000007','2km800000006',\n",
    "              '2kxn00000007','2kxn00000008','2kxn00000001',\n",
    "              '2l2k00000006','2l2k00000002','2l2k00000007',\n",
    "              '2l3j00000008','2l3j00000001','2l3j00000002',\n",
    "              '2l5d00000004','2l5d00000016','2l5d00000008',\n",
    "              '2lbs00000013','2lbs00000009','2lbs00000005',\n",
    "              '2leb00000018','2leb00000000','2leb00000016',\n",
    "              '2lec00000018','2lec00000002','2lec00000007',\n",
    "              '2m8d00000013','2m8d00000003','2m8d00000010',\n",
    "              '2mb000000004','2mb000000018','2mb000000001',\n",
    "              '2mfc00000005','2mfc00000001','2mfc00000015',\n",
    "              '2mfe00000001','2mfe00000002','2mfe00000013',\n",
    "              '2mgz00000017','2mgz00000004','2mgz00000009',\n",
    "              '2mjh00000019','2mjh00000006','2mjh00000009',\n",
    "              '2mki00000005','2mki00000014','2mki00000002',\n",
    "              '2mkk00000006','2mkk00000008','2mkk00000004',\n",
    "              '2mz100000018','2mz100000004','2mz100000003',\n",
    "              '2n7c00000002','2n7c00000010','2n7c00000007',\n",
    "              '2n8l00000003','2n8l00000006','2n8l00000004',\n",
    "              '2rra00000005','2rra00000008','2rra00000009',\n",
    "              '2rs200000018','2rs200000004','2rs200000017',\n",
    "              '2ru300000015','2ru300000011','2ru300000018',\n",
    "              '4cio00000000','4cio00000006','4cio00000008',\n",
    "              '5m8i00000008','5m8i00000014','5m8i00000006',\n",
    "              '5mpg00000011','5mpg00000007','5mpg00000003',\n",
    "              '5mpl00000004','5mpl00000012','5mpl00000002',\n",
    "              '5n8l00000014','5n8l00000018','5n8l00000013',\n",
    "              '5n8m00000015','5n8m00000004','5n8m00000002',\n",
    "              '5x3z00000016','5x3z00000010','5x3z00000001',\n",
    "              '6gbm00000002','6gbm00000000','6gbm00000011',\n",
    "              '6hpj00000013','6hpj00000006','6hpj00000012',\n",
    "              '6snj00000009','6snj00000000','6snj00000002',\n",
    "              '6tph00000004','6tph00000009','6tph00000001',\n",
    "              '7act00000009','7act00000008','7act00000000',\n",
    "              '1t2r00000001','1t2r00000004','1t2r00000009', \n",
    "              '4bs200000006', '4bs200000007', '4bs200000009', \n",
    "              '2i2y00000001', '2i2y00000004', '2i2y00000014', \n",
    "              '2li800000004', '2li800000009', '2li800000015', \n",
    "              '2yh100000003', '2yh100000006', '2yh100000007', \n",
    "              '2hgh00000006', '2hgh00000010', '2hgh00000013', \n",
    "              '2ese00000001', '2ese00000004', '2ese00000011', \n",
    "              '6sdw00000002', '6sdw00000015', '6sdw00000016', \n",
    "              '2rqc00000002', '2rqc00000007', '2rqc00000018', \n",
    "              '1rkj00000001', '1rkj00000006', '1rkj00000009', \n",
    "              '6sdy00000006', '6sdy00000009', '6sdy00000017', \n",
    "              '6wlh00000007', '6wlh00000016', '6wlh00000017', \n",
    "              '4b8t00000005', '4b8t00000013', '4b8t00000015', \n",
    "              '7acs00000006', '7acs00000011', '7acs00000013'\n",
    " ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Options\n",
    "\n",
    "The cell below will define 9 subfolds with around the same datasize for each task. A 3-fold cross validation will be done with each cross fold containing 3 sub fold. In each training cycle 2 subfolds are resserved for validation 1 for testing; the remaining 6 for training. Some options are\n",
    "\n",
    "* Task. `User_Task = \"SXPR\"`.\n",
    "* Number of cross folds to be done. We recommend `n_CrossFold = 9`.\n",
    "* Extent of external symmetry (BC percent) to be considered when we separate folds. We recommend `ClanGraphBcPercent = 90`, but 70 seems also affordable. (TODO Check)\n",
    "* Hierarchy of class labels. We recommend a two level hierarchy `TaskNameLabelLogicDict = {\"SXPR\":LabelLogic_level0, \"AUCG\": LabelLogic_level1,}`, but a finer hierarchy `commandDataFetcher.OBSOLETE_TaskNameLabelLogicDict` is also provided if needed.\n",
    "* Filter using Derived Data from PDB FTP. We recommend filtering as suggested in `Df_grand`.\n",
    "\n",
    "Some options are machine learning specific hyperparameters and can be tuned in combination if desired. See comments for detail. Some worth mentioning hyperparameters:\n",
    "* Noise in input/hidden layer.\n",
    "* Ghost Batch Normalisation. As the size of dataset grow we can no longer afford small-batch-size (typically 128 or less datapoint) training. A remedy popularised in recent year is GBN.\n",
    "* Multi-step cosine scheduler. `SimpleMultistepCosineLRS` This helps to propose multiple models ready for random forest settings.\n",
    "* Label smoothing by neighborhood.\n",
    "* Label smoothing by class.\n",
    "* Implementation of Bottleneck. This also allow width tuning as in wideresnet. \n",
    "\n",
    "Some further remarks \n",
    "\n",
    "* When we pack clans of different sizes into the cross folds, we are not aiming at a [bin-packing solution](https://en.wikipedia.org/wiki/Bin_packing_problem), but rather we aim at distributing clans of different sizes evenly among folds. The process will produce a dataframe `TaskClanFoldDf_BC{bc percent}.pkl`, that indicates which pdbids to be included in the fold. \n",
    "* While we cannot load all data into RAM, we will make 6 pass from Storage to RAM, where each pass is restricted to hold `User_DesiredBatchDatasize = 3500000` datapoint. \n",
    "* Resampling will be done in minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3072, 0.12, False, 'gelu', 16, 0.0001, 5000, 0.03, 1e-06, 0.7, 1e-06, 40, 0.01, 'SimpleMultistepCosineLRS_SXPR', True, 1.0, 0.0, False, 20, 1.0, 0.0, 0.01, 'CrossEntropyLoss', 0.25, 2.0, 100000.0)]\n"
     ]
    }
   ],
   "source": [
    "n_CrossFold = 9\n",
    "ClanGraphBcPercent = 30\n",
    "User_featuretype = 'altman'\n",
    "\n",
    "User_Task = \"SXPR\"\n",
    "n_row_maxhold = 10000\n",
    "\n",
    "\n",
    "# ================ Collapse. Click Please \n",
    "\n",
    "User_DesiredBatchDatasize    = 70000000 # NOTE This controls the number of new batch of dataset-dataloader being reloaded into memory\n",
    "User_SampleSizePerEpoch_Factor = 1.0 # NOTE This controls how much sample enters into an epoch. if < 1.0, the sampler will make less than User_DesiredBatchDatasize sample to be fed in one epoch\n",
    "\n",
    "User_SampleSizePerEpoch = int(User_DesiredBatchDatasize * User_SampleSizePerEpoch_Factor)\n",
    "n_datasetworker = 16\n",
    "#User_ExperiementName = 'SXPR-9CV'\n",
    "User_ExperiementName = 'SXPR-9CVMm%s'%(ClanGraphBcPercent)\n",
    "\n",
    "DIR_TrainingRoot = \"/home/homingla/Project-NucleicNet/Models/\"\n",
    "DIR_TrainLog = \"/home/homingla/Project-NucleicNet/Models/\" \n",
    "#DIR_Checkpoint = \"/home/homingla/Project-NucleicNet/Models/AUCG_Resnet50Pretrained/lightning_logs/version_4/checkpoints/epoch=4-step=4689.ckpt\"\n",
    "pl.seed_everything(42)\n",
    "Combination_SizeMinibatch = [3072]                  # NOTE We have used Ghost Batch Norm with virtual batch size 128\n",
    "Combination_LabelSmoothing  = [0.12]                # NOTE Default 0.12 when User_NeighborLabelSmoothAngstrom > 0.0. else 0.36\n",
    "Combination_PerformReduction = [False]              # NOTE Default False. True worsen the performance.\n",
    "Combination_Activation = ['gelu']                   # NOTE Default gelu \n",
    "Combination_n_ResnetBlock = [16]                    # NOTE Default 16 96 ok but lr tune needed\n",
    "Combination_lr = [1e-3  * 0.1]#,  1e-3  * 0.5, 1e-3 * 1.0,  1e-3  * 2.0 ]                          # NOTE Default 1e-3 /2 in SXPR. MMseq lower it \n",
    "Combination_min_lr = [1e-6]                        # NOTE Default 1e-6\n",
    "Combination_CooldownInterval = [5000]               # NOTE Default 2000\n",
    "Combination_AdamW_weight_decay = [0.01 * 3]        # NOTE Default model can tolerate 0.05 but not 0.1. In general 0.01-0.05 are satisfactory. Check Max Performance\n",
    "Combination_Dropoutp = [0.7]                    # NOTE Default 0.7 model can tolerate 0.7\n",
    "Combination_AddL1 = [0.000001]                      # NOTE Default 0 0.0001 poorer than 0.000001 \n",
    "Combination_n_channelbottleneck = [40]          # NOTE Default 40, but 160 leads to simpler model as indicated by L1 of weights? Check\n",
    "Combination_ShiftLrRatio = [0.01]                   # NOTE Unused\n",
    "Combination_User_LrScheduler = [\"SimpleMultistepCosineLRS_SXPR\"]           # NOTE Default SimpleMultistepCosineLRS CosineAnnealingLR DescendingCosineAnnealingLR_HalfEpoch\n",
    "Combination_User_BiasInSuffixFc = [True]            # NOTE Default True\n",
    "Combination_User_NoiseX = [0.125 *8]                # NOTE Default 1.0 model can tolerate 1.0-1.5\n",
    "Combination_User_NoiseY = [0.0]                     # NOTE Unused\n",
    "Combination_User_Mixup = [False]                    # NOTE Unused. \n",
    "Combination_User_NumReductionComponent = [20]       # NOTE Default. Unused unless PerformReduction = True\n",
    "Combination_User_NoiseZ = [0.125 *8]                # NOTE Default 1.5\n",
    "Combination_User_NeighborLabelSmoothAngstrom = [0.0] # NOTE Default 0.0. \n",
    "Combination_User_InputDropoutp = [0.01]             # NOTE Default 0.1 finalise after tuning all hyperparameters\n",
    "Combination_User_Loss = [\"CrossEntropyLoss\"]        # NOTE CrossEntropyLoss \n",
    "\n",
    "Combination_User_FocalLossAlpha = [0.25]            # NOTE Default 0.25 No effect if focal loss not used.\n",
    "Combination_User_FocalLossGamma = [2.0]             # NOTE Default 2. Note gamma == 0 returns CE\n",
    "Combination_User_GradientClippingValue = [1e5] # clip gradients' global norm to <= this number larger network may need larger clip? default 10000 TODO Test\n",
    "combinations = [\n",
    "                Combination_SizeMinibatch,\n",
    "                Combination_LabelSmoothing,\n",
    "                Combination_PerformReduction,\n",
    "                Combination_Activation,\n",
    "\n",
    "                Combination_n_ResnetBlock,\n",
    "                Combination_lr,\n",
    "                Combination_CooldownInterval,\n",
    "                Combination_AdamW_weight_decay,\n",
    "                Combination_min_lr,\n",
    "                Combination_Dropoutp,\n",
    "                Combination_AddL1,\n",
    "                Combination_n_channelbottleneck,\n",
    "                Combination_ShiftLrRatio,\n",
    "                Combination_User_LrScheduler,\n",
    "                Combination_User_BiasInSuffixFc,\n",
    "                Combination_User_NoiseX,\n",
    "                Combination_User_NoiseY,\n",
    "                Combination_User_Mixup,\n",
    "                Combination_User_NumReductionComponent,\n",
    "                Combination_User_NoiseZ,\n",
    "                Combination_User_NeighborLabelSmoothAngstrom,\n",
    "                Combination_User_InputDropoutp,\n",
    "                Combination_User_Loss,\n",
    "                Combination_User_FocalLossAlpha,\n",
    "                Combination_User_FocalLossGamma,\n",
    "                Combination_User_GradientClippingValue,\n",
    "                ]\n",
    "\n",
    "# result contains all possible combinations.\n",
    "CombinationList = list(itertools.product(*combinations))\n",
    "print(CombinationList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SXPR': {'Base': {'union': ['A', 'U', 'C', 'G'], 'exclu': [], 'intersect': []}, 'Nonsite': {'union': ['nonsite_'], 'exclu': ['F'], 'intersect': []}, 'P': {'union': ['P'], 'exclu': [], 'intersect': []}, 'R': {'union': ['R'], 'exclu': [], 'intersect': []}}, 'AUCG': {'A': {'union': ['A'], 'exclu': [], 'intersect': ['nucsite_']}, 'U': {'union': ['U'], 'exclu': [], 'intersect': ['nucsite_']}, 'C': {'union': ['C'], 'exclu': [], 'intersect': ['nucsite_']}, 'G': {'union': ['G'], 'exclu': [], 'intersect': ['nucsite_']}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ========================= Auto \n",
    "\n",
    "\n",
    "FetchTaskC = FetchTask(DIR_DerivedData = DIR_DerivedData,\n",
    "                              DIR_Typi = DIR_Typi,\n",
    "                              DIR_FuelInput = DIR_FuelInput,\n",
    "                              Df_grand = Df_grand,\n",
    "                              TaskNameLabelLogicDict = None,\n",
    "                              n_row_maxhold = n_row_maxhold)\n",
    "\n",
    "# =========================\n",
    "# Get Definition of Tasks\n",
    "# =========================\n",
    "# NOTE This collects task name and how to get corresponding data in typi \n",
    "TaskNameLabelLogicDict = FetchTaskC.Return_TaskNameLabelLogicDict()\n",
    "#print(TaskNameLabelLogicDict)\n",
    "\n",
    "\n",
    "print(FetchTaskC.TaskNameLabelLogicDict)\n",
    "\n",
    "# =======================\n",
    "# Task Clan Fold Dataframe\n",
    "# ======================= \n",
    "# NOTE each element contains 3 tuple train val test\n",
    "CrossFoldDfList = FetchTaskC.Return_CrossFoldDfList(n_CrossFold = n_CrossFold, \n",
    "                                                      ClanGraphBcPercent = ClanGraphBcPercent, \n",
    "                                                      User_Task = User_Task,\n",
    "                                                      Factor_ClampOnMaxSize = 450000,  # NOTE Constraint on datasize of a clan. For SXPR, this is raised as the number of datapoint per entry is much larger.\n",
    "                                                      Factor_ClampOnMultistate = 20,   # NOTE Constriant on number of multistate file read\n",
    "                                                      NmrStates = NmrStates\n",
    "                                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 0.12, False, 'gelu', 16, 0.0001, 5000, 0.03, 1e-06, 0.7, 1e-06, 40, 0.01, 'SimpleMultistepCosineLRS_SXPR', True, 1.0, 0.0, False, 20, 1.0, 0.0, 0.01, 'CrossEntropyLoss', 0.25, 2.0, 100000.0)\n",
      "Getting TrainValTest batches\n",
      "521 187 81 789\n",
      "Concating Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521/521 [00:00<00:00, 2608.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Concat data. Cooling down\n",
      "2203891 2203891\n",
      "{0: 4.110910548966622, 1: 5.259288621497391, 2: 4.284985044121195, 3: 3.920007379614112}\n",
      "Concating Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [00:00<00:00, 1903.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Concat data. Cooling down\n",
      "1153091 1153091\n",
      "{0: 1.3187801873485685, 1: 5.399679784269039, 2: 1.2747926484975125, 3: 1.421290549808437}\n",
      "Training model constr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type                       | Params\n",
      "-------------------------------------------------------------\n",
      "0 | nested_module | B1hw_LayerResnetBottleneck | 311 K \n",
      "1 | prefix_layerD | Sequential                 | 0     \n",
      "2 | suffix_layerA | Sequential                 | 0     \n",
      "3 | suffix_layerD | Sequential                 | 230 K \n",
      "4 | suffix_layerZ | Sequential                 | 1.9 K \n",
      "5 | loss          | CrossEntropyLoss           | 0     \n",
      "-------------------------------------------------------------\n",
      "543 K     Trainable params\n",
      "0         Non-trainable params\n",
      "543 K     Total params\n",
      "2.175     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/homingla/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:633: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  44%|████▍     | 12727/28886 [3:02:33<3:51:47,  1.16it/s, loss=0.982, v_num=0_1, train_loss_s=0.981, val_loss_s=1.280]10000 20000 9.900000000000001e-05\n",
      "Epoch 2: 100%|██████████| 28886/28886 [6:43:46<00:00,  1.19it/s, loss=0.818, v_num=0_1, train_loss_s=0.829, val_loss_s=1.390]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  7.3207e+04     \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  2.4399e+04     \t|3              \t|  7.3197e+04     \t|  99.987         \t|\n",
      "run_training_batch                 \t|  0.91742        \t|68358          \t|  6.2713e+04     \t|  85.666         \t|\n",
      "optimizer_step_with_closure_0      \t|  0.91491        \t|68358          \t|  6.2541e+04     \t|  85.431         \t|\n",
      "training_step_and_backward         \t|  0.71582        \t|68358          \t|  4.8932e+04     \t|  66.841         \t|\n",
      "backward                           \t|  0.58509        \t|68358          \t|  3.9996e+04     \t|  54.634         \t|\n",
      "model_forward                      \t|  0.1287         \t|68358          \t|  8798.0         \t|  12.018         \t|\n",
      "training_step                      \t|  0.12846        \t|68358          \t|  8781.2         \t|  11.995         \t|\n",
      "get_validate_batch                 \t|  0.22719        \t|18600          \t|  4225.7         \t|  5.7722         \t|\n",
      "fetch_next_validate_batch          \t|  0.22714        \t|18600          \t|  4224.7         \t|  5.771          \t|\n",
      "evaluation_step_and_end            \t|  0.22296        \t|18302          \t|  4080.7         \t|  5.5742         \t|\n",
      "validation_step                    \t|  0.22283        \t|18302          \t|  4078.3         \t|  5.5709         \t|\n",
      "on_train_batch_end                 \t|  0.0037068      \t|68358          \t|  253.39         \t|  0.34612        \t|\n",
      "on_train_batch_start               \t|  0.0024113      \t|68358          \t|  164.83         \t|  0.22516        \t|\n",
      "zero_grad                          \t|  0.0019765      \t|68358          \t|  135.11         \t|  0.18456        \t|\n",
      "get_train_batch                    \t|  0.0017474      \t|68361          \t|  119.45         \t|  0.16317        \t|\n",
      "fetch_next_train_batch             \t|  0.0017151      \t|68361          \t|  117.25         \t|  0.16016        \t|\n",
      "on_validation_batch_end            \t|  0.0036487      \t|18302          \t|  66.778         \t|  0.091219       \t|\n",
      "training_batch_to_device           \t|  0.00043971     \t|68358          \t|  30.057         \t|  0.041058       \t|\n",
      "on_validation_end                  \t|  0.077883       \t|301            \t|  23.443         \t|  0.032023       \t|\n",
      "evaluation_batch_to_device         \t|  0.0010721      \t|18302          \t|  19.622         \t|  0.026803       \t|\n",
      "on_after_backward                  \t|  5.1811e-05     \t|68358          \t|  3.5417         \t|  0.004838       \t|\n",
      "on_batch_start                     \t|  4.2759e-05     \t|68358          \t|  2.9229         \t|  0.0039927      \t|\n",
      "on_batch_end                       \t|  4.0657e-05     \t|68358          \t|  2.7792         \t|  0.0037964      \t|\n",
      "on_before_backward                 \t|  3.7067e-05     \t|68358          \t|  2.5338         \t|  0.0034612      \t|\n",
      "on_before_optimizer_step           \t|  3.6739e-05     \t|68358          \t|  2.5114         \t|  0.0034306      \t|\n",
      "on_before_zero_grad                \t|  3.6543e-05     \t|68358          \t|  2.498          \t|  0.0034122      \t|\n",
      "on_validation_start                \t|  0.0054499      \t|301            \t|  1.6404         \t|  0.0022408      \t|\n",
      "on_validation_model_eval           \t|  0.0053053      \t|301            \t|  1.5969         \t|  0.0021813      \t|\n",
      "on_validation_batch_start          \t|  5.0262e-05     \t|18302          \t|  0.91989        \t|  0.0012566      \t|\n",
      "training_step_end                  \t|  9.0804e-06     \t|68358          \t|  0.62072        \t|  0.00084789     \t|\n",
      "get_sanity_check_batch             \t|  0.17673        \t|3              \t|  0.53019        \t|  0.00072424     \t|\n",
      "fetch_next_sanity_check_batch      \t|  0.17665        \t|3              \t|  0.52995        \t|  0.00072391     \t|\n",
      "validation_step_end                \t|  1.1309e-05     \t|18302          \t|  0.20698        \t|  0.00028274     \t|\n",
      "on_train_start                     \t|  0.036182       \t|1              \t|  0.036182       \t|  4.9424e-05     \t|\n",
      "on_validation_epoch_end            \t|  7.9622e-05     \t|301            \t|  0.023966       \t|  3.2738e-05     \t|\n",
      "on_pretrain_routine_start          \t|  0.021687       \t|1              \t|  0.021687       \t|  2.9624e-05     \t|\n",
      "on_epoch_end                       \t|  4.3716e-05     \t|304            \t|  0.01329        \t|  1.8154e-05     \t|\n",
      "on_epoch_start                     \t|  3.3936e-05     \t|304            \t|  0.010317       \t|  1.4093e-05     \t|\n",
      "on_validation_epoch_start          \t|  3.3476e-05     \t|301            \t|  0.010076       \t|  1.3764e-05     \t|\n",
      "on_train_epoch_start               \t|  0.0027878      \t|3              \t|  0.0083635      \t|  1.1425e-05     \t|\n",
      "on_train_epoch_end                 \t|  0.0014302      \t|3              \t|  0.0042907      \t|  5.861e-06      \t|\n",
      "configure_optimizers               \t|  0.0040069      \t|1              \t|  0.0040069      \t|  5.4734e-06     \t|\n",
      "on_sanity_check_start              \t|  0.0016675      \t|1              \t|  0.0016675      \t|  2.2778e-06     \t|\n",
      "on_train_end                       \t|  0.00121        \t|1              \t|  0.00121        \t|  1.6529e-06     \t|\n",
      "on_sanity_check_end                \t|  5.3154e-05     \t|1              \t|  5.3154e-05     \t|  7.2608e-08     \t|\n",
      "on_pretrain_routine_end            \t|  4.7977e-05     \t|1              \t|  4.7977e-05     \t|  6.5536e-08     \t|\n",
      "on_fit_end                         \t|  4.7184e-05     \t|1              \t|  4.7184e-05     \t|  6.4453e-08     \t|\n",
      "on_configure_sharded_model         \t|  4.5663e-05     \t|1              \t|  4.5663e-05     \t|  6.2375e-08     \t|\n",
      "on_before_accelerator_backend_setup\t|  4.4483e-05     \t|1              \t|  4.4483e-05     \t|  6.0764e-08     \t|\n",
      "setup                              \t|  4.3131e-05     \t|1              \t|  4.3131e-05     \t|  5.8917e-08     \t|\n",
      "on_fit_start                       \t|  3.969e-05      \t|1              \t|  3.969e-05      \t|  5.4216e-08     \t|\n",
      "teardown                           \t|  3.8904e-05     \t|1              \t|  3.8904e-05     \t|  5.3143e-08     \t|\n",
      "configure_callbacks                \t|  2.6913e-05     \t|1              \t|  2.6913e-05     \t|  3.6763e-08     \t|\n",
      "on_train_dataloader                \t|  1.5656e-05     \t|1              \t|  1.5656e-05     \t|  2.1386e-08     \t|\n",
      "prepare_data                       \t|  1.1294e-05     \t|1              \t|  1.1294e-05     \t|  1.5428e-08     \t|\n",
      "on_val_dataloader                  \t|  8.058e-06      \t|1              \t|  8.058e-06      \t|  1.1007e-08     \t|\n",
      "configure_sharded_model            \t|  8.03e-06       \t|1              \t|  8.03e-06       \t|  1.0969e-08     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 0.12, False, 'gelu', 16, 0.0001, 5000, 0.03, 1e-06, 0.7, 1e-06, 40, 0.01, 'SimpleMultistepCosineLRS_SXPR', True, 1.0, 0.0, False, 20, 1.0, 0.0, 0.01, 'CrossEntropyLoss', 0.25, 2.0, 100000.0)\n",
      "Getting TrainValTest batches\n",
      "511 199 79 789\n",
      "Concating Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 511/511 [00:00<00:00, 2890.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Concat data. Cooling down\n",
      "2065217 2065217\n",
      "{0: 4.301407992089992, 1: 5.259050975031245, 2: 4.535539609191034, 3: 4.493224291003407}\n",
      "Concating Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:00<00:00, 1294.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Concat data. Cooling down\n",
      "1404937 1404937\n",
      "{0: 1.37760664733385, 1: 5.289587737626559, 2: 1.4856519625597484, 3: 1.251617914365977}\n",
      "Training model constr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type                       | Params\n",
      "-------------------------------------------------------------\n",
      "0 | nested_module | B1hw_LayerResnetBottleneck | 311 K \n",
      "1 | prefix_layerD | Sequential                 | 0     \n",
      "2 | suffix_layerA | Sequential                 | 0     \n",
      "3 | suffix_layerD | Sequential                 | 230 K \n",
      "4 | suffix_layerZ | Sequential                 | 1.9 K \n",
      "5 | loss          | CrossEntropyLoss           | 0     \n",
      "-------------------------------------------------------------\n",
      "543 K     Trainable params\n",
      "0         Non-trainable params\n",
      "543 K     Total params\n",
      "2.175     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  44%|████▍     | 12243/27786 [2:56:37<3:44:14,  1.16it/s, loss=0.988, v_num=2_3, train_loss_s=0.993, val_loss_s=1.250]10000 20000 9.900000000000001e-05\n",
      "Epoch 2: 100%|██████████| 27786/27786 [6:44:13<00:00,  1.15it/s, loss=0.81, v_num=2_3, train_loss_s=0.789, val_loss_s=1.330]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  7.2612e+04     \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  2.4201e+04     \t|3              \t|  7.2604e+04     \t|  99.989         \t|\n",
      "run_training_batch                 \t|  0.90648        \t|68358          \t|  6.1965e+04     \t|  85.338         \t|\n",
      "optimizer_step_with_closure_0      \t|  0.904          \t|68358          \t|  6.1796e+04     \t|  85.105         \t|\n",
      "training_step_and_backward         \t|  0.70812        \t|68358          \t|  4.8405e+04     \t|  66.663         \t|\n",
      "backward                           \t|  0.57619        \t|68358          \t|  3.9387e+04     \t|  54.244         \t|\n",
      "model_forward                      \t|  0.12989        \t|68358          \t|  8878.8         \t|  12.228         \t|\n",
      "training_step                      \t|  0.12964        \t|68358          \t|  8861.8         \t|  12.204         \t|\n",
      "get_validate_batch                 \t|  0.30207        \t|15300          \t|  4621.6         \t|  6.3648         \t|\n",
      "fetch_next_validate_batch          \t|  0.30202        \t|15300          \t|  4620.9         \t|  6.3638         \t|\n",
      "evaluation_step_and_end            \t|  0.25728        \t|15002          \t|  3859.7         \t|  5.3156         \t|\n",
      "validation_step                    \t|  0.25715        \t|15002          \t|  3857.8         \t|  5.3129         \t|\n",
      "on_train_batch_end                 \t|  0.0037292      \t|68358          \t|  254.92         \t|  0.35108        \t|\n",
      "on_train_batch_start               \t|  0.0024277      \t|68358          \t|  165.95         \t|  0.22854        \t|\n",
      "zero_grad                          \t|  0.0019876      \t|68358          \t|  135.87         \t|  0.18712        \t|\n",
      "get_train_batch                    \t|  0.0017291      \t|68361          \t|  118.2          \t|  0.16279        \t|\n",
      "fetch_next_train_batch             \t|  0.0016963      \t|68361          \t|  115.96         \t|  0.1597         \t|\n",
      "on_validation_batch_end            \t|  0.0035487      \t|15002          \t|  53.238         \t|  0.073319       \t|\n",
      "training_batch_to_device           \t|  0.00044388     \t|68358          \t|  30.343         \t|  0.041788       \t|\n",
      "on_validation_end                  \t|  0.056969       \t|301            \t|  17.148         \t|  0.023615       \t|\n",
      "evaluation_batch_to_device         \t|  0.00074839     \t|15002          \t|  11.227         \t|  0.015462       \t|\n",
      "on_after_backward                  \t|  5.089e-05      \t|68358          \t|  3.4787         \t|  0.0047908      \t|\n",
      "on_batch_start                     \t|  4.3052e-05     \t|68358          \t|  2.943          \t|  0.004053       \t|\n",
      "on_batch_end                       \t|  4.1747e-05     \t|68358          \t|  2.8537         \t|  0.0039301      \t|\n",
      "on_before_optimizer_step           \t|  3.897e-05      \t|68358          \t|  2.6639         \t|  0.0036687      \t|\n",
      "on_before_backward                 \t|  3.7118e-05     \t|68358          \t|  2.5373         \t|  0.0034944      \t|\n",
      "on_before_zero_grad                \t|  3.6947e-05     \t|68358          \t|  2.5256         \t|  0.0034782      \t|\n",
      "on_validation_model_eval           \t|  0.0055206      \t|301            \t|  1.6617         \t|  0.0022885      \t|\n",
      "on_validation_start                \t|  0.0050091      \t|301            \t|  1.5077         \t|  0.0020764      \t|\n",
      "get_sanity_check_batch             \t|  0.26945        \t|3              \t|  0.80836        \t|  0.0011133      \t|\n",
      "fetch_next_sanity_check_batch      \t|  0.26939        \t|3              \t|  0.80817        \t|  0.001113       \t|\n",
      "on_validation_batch_start          \t|  4.3813e-05     \t|15002          \t|  0.65729        \t|  0.0009052      \t|\n",
      "training_step_end                  \t|  9.1648e-06     \t|68358          \t|  0.62649        \t|  0.00086279     \t|\n",
      "validation_step_end                \t|  1.1325e-05     \t|15002          \t|  0.16989        \t|  0.00023397     \t|\n",
      "on_train_start                     \t|  0.044035       \t|1              \t|  0.044035       \t|  6.0645e-05     \t|\n",
      "on_pretrain_routine_start          \t|  0.024176       \t|1              \t|  0.024176       \t|  3.3295e-05     \t|\n",
      "on_validation_epoch_end            \t|  7.0726e-05     \t|301            \t|  0.021289       \t|  2.9318e-05     \t|\n",
      "on_epoch_end                       \t|  4.4345e-05     \t|304            \t|  0.013481       \t|  1.8566e-05     \t|\n",
      "on_epoch_start                     \t|  3.5349e-05     \t|304            \t|  0.010746       \t|  1.48e-05       \t|\n",
      "on_validation_epoch_start          \t|  3.4741e-05     \t|301            \t|  0.010457       \t|  1.4401e-05     \t|\n",
      "on_train_epoch_start               \t|  0.0033336      \t|3              \t|  0.010001       \t|  1.3773e-05     \t|\n",
      "configure_optimizers               \t|  0.0042407      \t|1              \t|  0.0042407      \t|  5.8403e-06     \t|\n",
      "on_train_epoch_end                 \t|  0.0013694      \t|3              \t|  0.0041082      \t|  5.6578e-06     \t|\n",
      "on_sanity_check_start              \t|  0.0016755      \t|1              \t|  0.0016755      \t|  2.3074e-06     \t|\n",
      "on_train_end                       \t|  0.00093352     \t|1              \t|  0.00093352     \t|  1.2856e-06     \t|\n",
      "on_sanity_check_end                \t|  6.818e-05      \t|1              \t|  6.818e-05      \t|  9.3897e-08     \t|\n",
      "on_pretrain_routine_end            \t|  5.8899e-05     \t|1              \t|  5.8899e-05     \t|  8.1115e-08     \t|\n",
      "on_fit_end                         \t|  4.8391e-05     \t|1              \t|  4.8391e-05     \t|  6.6643e-08     \t|\n",
      "on_configure_sharded_model         \t|  4.2849e-05     \t|1              \t|  4.2849e-05     \t|  5.9011e-08     \t|\n",
      "on_before_accelerator_backend_setup\t|  4.1149e-05     \t|1              \t|  4.1149e-05     \t|  5.667e-08      \t|\n",
      "teardown                           \t|  4.0148e-05     \t|1              \t|  4.0148e-05     \t|  5.5291e-08     \t|\n",
      "on_fit_start                       \t|  3.9721e-05     \t|1              \t|  3.9721e-05     \t|  5.4703e-08     \t|\n",
      "configure_sharded_model            \t|  3.5697e-05     \t|1              \t|  3.5697e-05     \t|  4.9161e-08     \t|\n",
      "setup                              \t|  3.3317e-05     \t|1              \t|  3.3317e-05     \t|  4.5884e-08     \t|\n",
      "on_train_dataloader                \t|  2.517e-05      \t|1              \t|  2.517e-05      \t|  3.4664e-08     \t|\n",
      "configure_callbacks                \t|  1.4883e-05     \t|1              \t|  1.4883e-05     \t|  2.0497e-08     \t|\n",
      "on_val_dataloader                  \t|  1.0372e-05     \t|1              \t|  1.0372e-05     \t|  1.4284e-08     \t|\n",
      "prepare_data                       \t|  7.274e-06      \t|1              \t|  7.274e-06      \t|  1.0018e-08     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 0.12, False, 'gelu', 16, 0.0001, 5000, 0.03, 1e-06, 0.7, 1e-06, 40, 0.01, 'SimpleMultistepCosineLRS_SXPR', True, 1.0, 0.0, False, 20, 1.0, 0.0, 0.01, 'CrossEntropyLoss', 0.25, 2.0, 100000.0)\n",
      "Getting TrainValTest batches\n",
      "546 163 80 789\n",
      "Concating Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [00:00<00:00, 4004.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Concat data. Cooling down\n",
      "2107102 2107102\n",
      "{0: 4.099299583658774, 1: 5.362551527576903, 2: 4.199043514154739, 3: 3.923616298754571}\n",
      "Concating Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:00<00:00, 2164.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Concat data. Cooling down\n",
      "1105842 1105842\n",
      "{0: 1.2766304912791007, 1: 5.2056452130287685, 2: 1.3796897422245078, 3: 1.2971584136251835}\n",
      "Training model constr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type                       | Params\n",
      "-------------------------------------------------------------\n",
      "0 | nested_module | B1hw_LayerResnetBottleneck | 311 K \n",
      "1 | prefix_layerD | Sequential                 | 0     \n",
      "2 | suffix_layerA | Sequential                 | 0     \n",
      "3 | suffix_layerD | Sequential                 | 230 K \n",
      "4 | suffix_layerZ | Sequential                 | 1.9 K \n",
      "5 | loss          | CrossEntropyLoss           | 0     \n",
      "-------------------------------------------------------------\n",
      "543 K     Trainable params\n",
      "0         Non-trainable params\n",
      "543 K     Total params\n",
      "2.175     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  44%|████▍     | 12859/29186 [2:55:49<3:43:14,  1.22it/s, loss=0.994, v_num=4_5, train_loss_s=1.010, val_loss_s=1.240]10000 20000 9.900000000000001e-05\n",
      "Epoch 2:  87%|████████▋ | 25433/29186 [5:49:50<51:37,  1.21it/s, loss=0.816, v_num=4_5, train_loss_s=0.808, val_loss_s=1.340]"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4889/3004548555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    239\u001b[0m                                                         User_GradientClippingValue = User_GradientClippingValue)\n\u001b[1;32m    240\u001b[0m     \u001b[0mtrainer00\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mtrainer00\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    736\u001b[0m             )\n\u001b[1;32m    737\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         )\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \"\"\"\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# SAVE METRICS TO LOGGERS AND PROGRESS_BAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# -----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_train_step_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\u001b[0m in \u001b[0;36mupdate_train_step_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_end_reached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_update_logs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_dev_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_train_epoch_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\u001b[0m in \u001b[0;36mlog_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# log actual metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_and_log_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/loggers/base.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mrank_zero_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;34m\"\"\"Save recorded hparams and metrics into files.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mhparams_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME_HPARAMS_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0msave_hparams_to_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36msave_hparams_to_yaml\u001b[0;34m(config_yaml, hparams, use_omegaconf)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;31m# saving the standard way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_yaml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams_allowed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/envs/Nucl/lib/python3.8/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  87%|████████▋ | 25433/29186 [5:50:05<51:39,  1.21it/s, loss=0.816, v_num=4_5, train_loss_s=0.808, val_loss_s=1.340]"
     ]
    }
   ],
   "source": [
    "for cccc in CombinationList:\n",
    "  for User_SelectedCrossFoldIndex in [0,3,6]:\n",
    "\n",
    "    print(cccc)\n",
    "    # ==========================\n",
    "    # Hyperparam \n",
    "    # ===============================\n",
    "    PART0_InitialiseHyperparameters = True\n",
    "    if PART0_InitialiseHyperparameters:\n",
    "    # ==========================\n",
    "    # Hyperparam \n",
    "    # ===============================\n",
    "\n",
    "        User_SizeMinibatch = cccc[0] #256 \n",
    "        User_LabelSmoothing = cccc[1] #0.16 \n",
    "        User_PerformReduction = cccc[2] #True \n",
    "        User_Activation = cccc[3] #'gelu'\n",
    "        User_n_ResnetBlock = cccc[4]#16 \n",
    "        User_lr = cccc[5] #1e-3      \n",
    "        n_Restart = 1  \n",
    "        User_CooldownInterval = cccc[6] #951\n",
    "        User_AdamW_weight_decay = cccc[7] #1e-2\n",
    "        User_min_lr = cccc[8] #1e-6\n",
    "\n",
    "\n",
    "        User_Dropoutp = cccc[9]\n",
    "        User_AddL1 = cccc[10]\n",
    "        User_n_channelbottleneck = cccc[11]\n",
    "        User_ShiftLrRatio = cccc[12]\n",
    "\n",
    "\n",
    "        # NOTE Currently fixed for benchmarking\n",
    "        User_LrScheduler = cccc[13]   \n",
    "        User_BiasInSuffixFc = cccc[14]\n",
    "        User_NoiseX = cccc[15]\n",
    "        User_NoiseY = cccc[16]\n",
    "        User_Mixup = cccc[17] # NOTE Not used.\n",
    "        User_NumReductionComponent = cccc[18]\n",
    "        User_NoiseZ = cccc[19]\n",
    "        User_NeighborLabelSmoothAngstrom = cccc[20]\n",
    "        User_InputDropoutp = cccc[21]\n",
    "        User_Loss = cccc[22]\n",
    "        User_FocalLossAlpha = cccc[23]\n",
    "        User_FocalLossGamma = cccc[24]\n",
    "        User_GradientClippingValue = cccc[25]\n",
    "        #print(User_GradientClippingValue)\n",
    "        #sys.exit()\n",
    "\n",
    "        FetchDatasetC = FetchDataset(\n",
    "            DIR_DerivedData = DIR_DerivedData,\n",
    "            DIR_Typi = DIR_Typi,\n",
    "            DIR_FuelInput = DIR_FuelInput,\n",
    "            User_DesiredDatasize    = User_DesiredBatchDatasize, # NOTE This controls the number of new batch of dataset-dataloader being reloaded into memory\n",
    "            User_SampleSizePerEpoch_Factor = User_SampleSizePerEpoch_Factor, # NOTE This controls how much sample enters into an epoch\n",
    "            User_featuretype = User_featuretype,\n",
    "            n_datasetworker = n_datasetworker,\n",
    "            ClanGraphBcPercent = ClanGraphBcPercent)\n",
    "\n",
    "        classindex_str = sorted(TaskNameLabelLogicDict[User_Task].keys()) \n",
    "        ClassName_ClassIndex_Dict = dict(zip(classindex_str, range(len(classindex_str))))\n",
    "\n",
    "    # ============================\n",
    "    # Get Cross-Folds and Batches\n",
    "    # ============================\n",
    "    print(\"Getting TrainValTest batches\")\n",
    "    PART1A_GetCrossFolds = True\n",
    "    if PART1A_GetCrossFolds:\n",
    "\n",
    "        # NOTE Pdbids, Datasize weight\n",
    "        Train_PdbidBatches, TrainFold_PdbidSamplingWeight = CrossFoldDfList[User_SelectedCrossFoldIndex][0]\n",
    "        Val_PdbidBatches, ValFold_PdbidSamplingWeight = CrossFoldDfList[User_SelectedCrossFoldIndex][1]\n",
    "        Testing_PdbidBatches,TestingFold_PdbidSamplingWeight  = CrossFoldDfList[User_SelectedCrossFoldIndex][2]\n",
    "\n",
    "        print(len(Train_PdbidBatches), len(Val_PdbidBatches), len(Testing_PdbidBatches), len(set(Testing_PdbidBatches+Val_PdbidBatches+Train_PdbidBatches)))\n",
    "        Train_PdbidWeight = dict(\n",
    "                TrainFold_PdbidSamplingWeight[[\"Pdbid\", \"PdbidSamplingWeight\"]].values.tolist()\n",
    "                )\n",
    "        Val_PdbidWeight = dict(\n",
    "                ValFold_PdbidSamplingWeight[[\"Pdbid\", \"PdbidSamplingWeight\"]].values.tolist()\n",
    "                )\n",
    "        Testing_PdbidWeight = dict(\n",
    "                TestingFold_PdbidSamplingWeight[[\"Pdbid\", \"PdbidSamplingWeight\"]].values.tolist()\n",
    "                )\n",
    "\n",
    "    if User_Task == \"AUCG\":\n",
    "        User_datastride = 1\n",
    "    else:\n",
    "        User_datastride = 30 # NOTE I cannot take in all the data in RAM >40GB. This only applies on nonsite as it's much larger than any other classes. Still 22GB of RAM.\n",
    "\n",
    "\n",
    "    PART1B_DatasetDataloader = True\n",
    "    if PART1B_DatasetDataloader:\n",
    "        # NOTE Train\n",
    "        ds_train, ds_train_samplingweight = FetchDatasetC.GetDataset(\n",
    "                        Assigned_PdbidBatch = Train_PdbidBatches,\n",
    "                        Assigned_PdbidWeight = Train_PdbidWeight,\n",
    "                        User_NumReductionComponent = User_NumReductionComponent,\n",
    "                        ClassName_ClassIndex_Dict = ClassName_ClassIndex_Dict,\n",
    "                        User_datastride = User_datastride,\n",
    "                        User_Task = User_Task,\n",
    "                        PerformZscoring = True, \n",
    "                        PerformReduction = User_PerformReduction,\n",
    "                        User_NeighborLabelSmoothAngstrom = User_NeighborLabelSmoothAngstrom \n",
    "                        )\n",
    "                        \n",
    "        train_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "                        ds_train_samplingweight, User_SampleSizePerEpoch, replacement=True)\n",
    "        train_loader  = torch.utils.data.DataLoader(ds_train, batch_size=User_SizeMinibatch, drop_last=True, num_workers=4, \n",
    "                                                            pin_memory=True,worker_init_fn=None, prefetch_factor=3, persistent_workers=False,\n",
    "                                                            sampler = train_sampler)\n",
    "\n",
    "        # NOTE Val\n",
    "        ds_val, ds_val_samplingweight = FetchDatasetC.GetDataset(\n",
    "                        Assigned_PdbidBatch = Val_PdbidBatches,\n",
    "                        Assigned_PdbidWeight = Val_PdbidWeight,\n",
    "                        User_NumReductionComponent = User_NumReductionComponent,\n",
    "                        ClassName_ClassIndex_Dict = ClassName_ClassIndex_Dict,\n",
    "                        User_Task = User_Task,\n",
    "                        User_datastride = 10,                                    # NOTE Memory problem forces us to do it!,\n",
    "                        PerformZscoring = True, \n",
    "                        PerformReduction = User_PerformReduction,\n",
    "                        User_NeighborLabelSmoothAngstrom = User_NeighborLabelSmoothAngstrom \n",
    "                        )\n",
    "        val_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "            ds_val_samplingweight, int(User_SampleSizePerEpoch/100), replacement=True)\n",
    "        val_loader          = torch.utils.data.DataLoader(ds_val, batch_size=int(ds_val.__len__()/100), drop_last=False, num_workers=4, \n",
    "                                                            pin_memory=True,worker_init_fn=None, prefetch_factor=3, persistent_workers=False,\n",
    "                                                            shuffle=False, sampler = val_sampler)  \n",
    "\n",
    "        #NOTE Test\n",
    "        \"\"\"\n",
    "        ds_testing, ds_testing_samplingweight = FetchDatasetC.GetDataset(\n",
    "                        Assigned_PdbidBatch = Testing_PdbidBatches,\n",
    "                        Assigned_PdbidWeight = Testing_PdbidWeight,\n",
    "                        User_NumReductionComponent = User_NumReductionComponent,\n",
    "                        ClassName_ClassIndex_Dict = ClassName_ClassIndex_Dict,\n",
    "                        User_Task = User_Task,\n",
    "                        PerformZscoring = True, \n",
    "                        PerformReduction = User_PerformReduction,\n",
    "                        )\n",
    "        \n",
    "        testing_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "            ds_testing_samplingweight, int(User_SampleSizePerEpoch/100), replacement=True)\n",
    "        testing_loader          = torch.utils.data.DataLoader(ds_testing, batch_size=int(ds_testing.__len__()/100), drop_last=False, \n",
    "                                                            num_workers=4, \n",
    "                                                            pin_memory=True,worker_init_fn=None, prefetch_factor=3, persistent_workers=False,\n",
    "                                                            shuffle=False, sampler = testing_sampler) \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #sys.exit()\n",
    "\n",
    "        \n",
    "    # =====================\n",
    "    # Define Model\n",
    "    # ======================\n",
    "    print(\"Training model constr\")\n",
    "    PART2_DefineModel = True\n",
    "    if PART2_DefineModel:\n",
    "        if User_PerformReduction:\n",
    "            n_FeatPerShell = User_NumReductionComponent\n",
    "            hw_product = n_FeatPerShell*6\n",
    "        else:\n",
    "            n_FeatPerShell = 80\n",
    "            hw_product = 80*6\n",
    "\n",
    "        model = NucleicNet.Burn.M1.B1hw_FcLogits(\n",
    "                        model   = NucleicNet.Burn.M1.B1hw_LayerResnetBottleneck(n_FeatPerShell = n_FeatPerShell, \n",
    "                                                    n_Shell = 6,\n",
    "                                                    n_ShellMix = 2,\n",
    "                                                    User_Activation = User_Activation,\n",
    "                                                    User_Block = \"B1hw_BlockPreActResnet\",\n",
    "                                                    n_Blocks = User_n_ResnetBlock,\n",
    "                                                    ManualInitiation = False,\n",
    "                                                    User_n_channelbottleneck = User_n_channelbottleneck,\n",
    "                                                    User_NoiseZ = User_NoiseZ,\n",
    "                                                    ),\n",
    "\n",
    "                        #loss    = customloss, \n",
    "                        User_Loss = User_Loss, \n",
    "                        n_class = 4,\n",
    "                        hw_product = hw_product,\n",
    "                        AddMultiLabelSoftMarginLoss = False, # TODO Worsen stuff? One-vs-all likely of no use.\n",
    "                        User_lr = User_lr,\n",
    "                        User_min_lr = User_min_lr,\n",
    "                        User_LrScheduler = User_LrScheduler,\n",
    "                        User_CooldownInterval = User_CooldownInterval,\n",
    "                        BiasInSuffixFc = User_BiasInSuffixFc, \n",
    "                        # NOTE some kwargs for hparam record\n",
    "                        User_SizeMinibatch = User_SizeMinibatch,\n",
    "                        User_LabelSmoothing = User_LabelSmoothing,\n",
    "                        User_PerformReduction = User_PerformReduction,\n",
    "                        User_n_ResnetBlock = User_n_ResnetBlock,\n",
    "                        User_AdamW_weight_decay = User_AdamW_weight_decay,\n",
    "                        User_Activation = User_Activation,\n",
    "                        User_SelectedCrossFoldIndex = User_SelectedCrossFoldIndex,\n",
    "                        User_Dropoutp = User_Dropoutp,\n",
    "                        User_AddL1 = User_AddL1,\n",
    "                        User_n_channelbottleneck = User_n_channelbottleneck,\n",
    "                        User_ShiftLrRatio = User_ShiftLrRatio,\n",
    "                        User_NoiseX = User_NoiseX,\n",
    "                        User_NoiseY = User_NoiseY,\n",
    "                        #User_Mixup = User_Mixup,\n",
    "                        User_NumReductionComponent = User_NumReductionComponent,\n",
    "                        User_NoiseZ = User_NoiseZ,\n",
    "                        User_PdbidTraining = Train_PdbidBatches,\n",
    "                        User_PdbidValidation = Val_PdbidBatches,\n",
    "                        User_PdbidTesting = Testing_PdbidBatches,\n",
    "                        User_InputDropoutp = User_InputDropoutp,\n",
    "                        User_FocalLossAlpha = User_FocalLossAlpha,\n",
    "                        User_FocalLossGamma = User_FocalLossGamma,\n",
    "                        User_n_CrossFold = n_CrossFold,\n",
    "                        User_ClanGraphBcPercent = ClanGraphBcPercent,\n",
    "                        User_Task = User_Task,\n",
    "                        User_NeighborLabelSmoothAngstrom = User_NeighborLabelSmoothAngstrom,\n",
    "                        User_GradientClippingValue = User_GradientClippingValue,\n",
    "                        User_datastride = User_datastride,\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "        NucleicNet.Burn.util.ResetAllParameters(model)\n",
    "\n",
    "\n",
    "\n",
    "    # ====================\n",
    "    # Stage 0 training\n",
    "    # ====================\n",
    "    print(\"Training fit\")\n",
    "    trainer00 = NucleicNet.Burn.util.DefaultTrainer00(DIR_TrainLog = DIR_TrainLog, \n",
    "                                                        DIR_TrainingRoot = DIR_TrainingRoot, \n",
    "                                                        User_ExperiementName = User_ExperiementName,\n",
    "                                                        User_SizeMinibatch = User_SizeMinibatch ,\n",
    "                                                        User_ShiftLrRatio = User_ShiftLrRatio,\n",
    "                                                        User_Mixup = User_Mixup,\n",
    "                                                        User_GradientClippingValue = User_GradientClippingValue)\n",
    "    trainer00.logger._log_graph = True \n",
    "    trainer00.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "\n",
    "\n",
    "    del model, trainer00\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue\n",
    "\n",
    "Remember to train AUCG before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f598b2da1a38c0fbc1071e0920cd7e6bcaf63d50063efd89e5e41621a15766f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('JQCB': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
