{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we document how to check if a new dataset is built with consistent parameters and how to lock the dataset in case we need to update it.\n",
    "\n",
    "\n",
    "## Basic Checksum\n",
    "In the cell below we can check if a new dataset is built with parameters consistent to the old standard dataset downloaded when you clone the github repo. Say for example, you are building a new set of landmarks and want to check if the same parameters were used as the provided dataset. Follow these steps\n",
    " \n",
    "1. Move an old reliable landmark e.g. `4f3t00000000.nucsite.landmark` somewhere else e.g. into a junk folder `/home/Downloads/`\n",
    "2. Make the new set together with 4f3t.\n",
    "3. Because 4f3t was recorded in `../Database-PDB/CHECKSUM_LandmarkNucsiteIntegrity.pkl` checking its content against the one recorded should tell if the same protocol has been used in generating the new set. \n",
    "\n",
    "The whole protocol takes more than 20 minutes to complete the check. It prints lists of discrepancy and returns the discrepancies. Refer to the script `commandChecksum.py` for details. Further remarks. \n",
    "* Extra files compare to Typi (Class labels) for Halo/Feature/Landmark. These are rejected entries after the landmark building process (e.g. artificial base). `['1a1v00000000', '1kdh00000000', '1m0600000000', '2f5500000000', '3af600000000', '3c5f00000001', '3f2100000000', '3f2200000000', '3f2300000000', '3iem00000000', '3vaf00000000', '3vaf00000001', '3vak00000000', '3vak00000001', '4r8i00000000', '4tu700000000', '4tu700000001', '4wb200000000', '4wb200000001', '4wb300000000', '4wb300000001', '6idg00000000', '6kcp00000000', '6kdi00000000', '6l9700000000', '6mdx00000000', '6mdx00000001', '6u6x00000000', '6u6x00000001', '6ycs00000000', '6ycs00000001']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from NucleicNet.DatasetBuilding.util import *\n",
    "from NucleicNet.DatasetBuilding.commandChecksum import Checksum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ChecksumC = Checksum()\n",
    "Discrepancy_Typi = ChecksumC.CheckTypi()\n",
    "Discrepancy_Halo = ChecksumC.CheckHalo()\n",
    "Discrepancy_Apo = ChecksumC.CheckApo()\n",
    "Discrepancy_Cleansed = ChecksumC.CheckCleansed()\n",
    "Discrepancy_LandmarkNucsite = ChecksumC.CheckLandmarkNucsite()\n",
    "Discrepancy_LandmarkFpocket = ChecksumC.CheckLandmarkFpocket()\n",
    "#Discrepancy_Feature = ChecksumC.CheckFeature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ==========================================================================================\n",
    "## :x: WARNING. DO NOT RUN AFTER THIS LINE UNLESS YOU WANT TO OVERWRITE THE CHECKSUM STORED.\n",
    "## ==========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12530/12530 [01:32<00:00, 135.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from scipy import sparse\n",
    "# TODO load all typi and make a dictionary for it \n",
    "\n",
    "PdbidList = sorted(glob.glob(\"../Database-PDB/typi/*.typi.npz\"))\n",
    "PdbidList = [i.split(\"/\")[-1].split(\".\")[0] for i in PdbidList]\n",
    "datasizedf = {}# []\n",
    "for pdbid_i in tqdm.tqdm(range(len(PdbidList))):\n",
    "                pdbid = PdbidList[pdbid_i]\n",
    "                with np.load(\"../Database-PDB/typi/%s.typi.npz\" %(pdbid)) as f:\n",
    "                        typi = sparse.csr_matrix((f['data'], f['indices'], f['indptr']), shape= f['shape'])\n",
    "                #print(typi.shape[0])\n",
    "                datasizedf[pdbid] = typi.shape[0]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"../Database-PDB/CHECKSUM_Datapoint.pkl\", \"wb\") as fn:\n",
    "    pickle.dump(datasizedf, fn,protocol=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checksum Typi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12530/12530 [01:48<00:00, 115.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from scipy import sparse\n",
    "PdbidList = sorted(glob.glob(\"../Database-PDB/typi/*.typi.npz\"))\n",
    "PdbidList = [i.split(\"/\")[-1].split(\".\")[0] for i in PdbidList]\n",
    "typiintegrity = {}# []\n",
    "for pdbid_i in tqdm.tqdm(range(len(PdbidList))):\n",
    "                pdbid = PdbidList[pdbid_i]\n",
    "                with np.load(\"../Database-PDB/typi/%s.typi.npz\" %(pdbid)) as f:\n",
    "                        typi = sparse.csr_matrix((f['data'], f['indices'], f['indptr']), shape= f['shape'])\n",
    "                typiintegrity[pdbid] = np.sum(typi, axis = 0) # TODO This should be done columnwise, because the row almost always sum to 1 when there is no ambiguity\n",
    "\n",
    "#print(datasizedf)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"../Database-PDB/CHECKSUM_TypiIntegrity.pkl\", \"wb\") as fn:\n",
    "    pickle.dump(typiintegrity, fn,protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checksum Halo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12561/12561 [01:13<00:00, 170.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "from scipy import sparse\n",
    "# TODO load all typi and make a dictionary for it \n",
    "# TODO typi, halo, feature\n",
    "# TODO Report as a list\n",
    "PdbidList = sorted(glob.glob(\"../Database-PDB/halo/*.halo*\")) \n",
    "PdbidList = sorted(set([i.split(\"/\")[-1].split(\".\")[0] for i in PdbidList]))\n",
    "Integrity_Halo = {}# []\n",
    "for pdbid_i in tqdm.tqdm(range(len(PdbidList))):\n",
    "    pdbid = PdbidList[pdbid_i]\n",
    "\n",
    "    ContentCheckXyz = os.path.getsize(\"../Database-PDB/halo/%s.haloxyz\" %(pdbid)) \n",
    "    ContentCheckTup = os.path.getsize(\"../Database-PDB/halo/%s.halotup\" %(pdbid))\n",
    "\n",
    "    with open(\"../Database-PDB/halo/%s.halotup\" %(pdbid), 'rb') as fn:\n",
    "        halonum, e1,e2,e3 = pickle.load(fn) \n",
    "    Integrity_Halo[pdbid] = np.array([halonum.shape[0], sum(e1), sum(e2), sum(e3), ContentCheckXyz, ContentCheckTup])\n",
    "\n",
    "import pickle\n",
    "with open(\"../Database-PDB/CHECKSUM_HaloIntegrity.pkl\", \"wb\") as fn:\n",
    "    pickle.dump(Integrity_Halo, fn,protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checksum Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIntegrity_Feature = {}\\nfor pdbid_i in tqdm.tqdm(range(len(PdbidList))):\\n        pdbid = PdbidList[pdbid_i]\\n        with np.load(\"../Database-PDB/feature/%s\" %(pdbid)) as f:\\n                feat = sparse.csr_matrix((f[\\'data\\'], f[\\'indices\\'], f[\\'indptr\\']), shape= f[\\'shape\\'])\\n        Integrity_Feature[pdbid] = np.sum(feat, axis = 0) # TODO This should be done columnwise, because the row almost always sum to 1 when there is no ambiguity\\n\\n\\nimport pickle\\nwith open(\"../Database-PDB/CHECKSUM_FeatureIntegrity.pkl\", \"wb\") as fn:\\n    pickle.dump(Integrity_Feature, fn,protocol=4)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "from scipy import sparse\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "PdbidList = sorted(glob.glob(\"../Database-PDB/feature/*.npz*\")) \n",
    "PdbidList = sorted(set([i.split(\"/\")[-1] for i in PdbidList]))\n",
    "pool = multiprocessing.Pool(12)\n",
    "\n",
    "def OOC_IntegrityFeature(pdbid):\n",
    "        with np.load(\"../Database-PDB/feature/%s\" %(pdbid)) as f:\n",
    "                feat = sparse.csr_matrix((f['data'], f['indices'], f['indptr']), shape= f['shape'])\n",
    "        Integrity_Feature_ = np.sum(feat, axis = 0) # TODO This should be done columnwise, because the row almost always sum to 1 when there is no ambiguity\n",
    "\n",
    "        return (pdbid, Integrity_Feature_)\n",
    "\n",
    "Integrity_Feature = pool.map(OOC_IntegrityFeature, PdbidList)\n",
    "Integrity_Feature = dict(Integrity_Feature)\n",
    "import pickle\n",
    "with open(\"../Database-PDB/CHECKSUM_FeatureIntegrity.pkl\", \"wb\") as fn:\n",
    "    pickle.dump(Integrity_Feature, fn,protocol=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checksum Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12533/12533 [00:31<00:00, 403.23it/s]\n",
      "100%|██████████| 12561/12561 [00:34<00:00, 361.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "PdbidList = sorted(glob.glob(\"../Database-PDB/landmark/*.nucsite.landmark\")) \n",
    "PdbidList = sorted(set([i.split(\"/\")[-1].split(\".\")[0] for i in PdbidList]))\n",
    "Integrity_LandmarkNucsite = {}\n",
    "for pdbid_i in tqdm.tqdm(range(len(PdbidList))):\n",
    "    pdbid = PdbidList[pdbid_i]\n",
    "    ContentCheckNucsite = os.path.getsize(\"../Database-PDB/landmark/%s.nucsite.landmark\" %(pdbid)) \n",
    "    df = pd.read_pickle(\"../Database-PDB/landmark/%s.nucsite.landmark\" %(pdbid))\n",
    "    Integrity_LandmarkNucsite[pdbid] = np.array([df['centroid_id'].sum() , df.shape[0], np.around(df['x_coord'].sum(),3), np.around(df['y_coord'].sum(),3), np.around(df['z_coord'].sum(),3)])\n",
    "\n",
    "import pickle\n",
    "with open(\"../Database-PDB/CHECKSUM_LandmarkNucsiteIntegrity.pkl\", \"wb\") as fn:\n",
    "    pickle.dump(Integrity_LandmarkNucsite, fn,protocol=4)\n",
    "\n",
    "# NOTE Fpocket is stochastic and I cannot fix the random seed.\n",
    "PdbidList = sorted(glob.glob(\"../Database-PDB/landmark/*.fpocket.landmark\")) \n",
    "PdbidList = sorted(set([i.split(\"/\")[-1].split(\".\")[0] for i in PdbidList]))\n",
    "Integrity_LandmarkFpocket = {}\n",
    "for pdbid_i in tqdm.tqdm(range(len(PdbidList))):\n",
    "    pdbid = PdbidList[pdbid_i]\n",
    "    ContentCheckFpocket = os.path.getsize(\"../Database-PDB/landmark/%s.fpocket.landmark\" %(pdbid)) \n",
    "    df = pd.read_pickle(\"../Database-PDB/landmark/%s.fpocket.landmark\" %(pdbid))\n",
    "    Integrity_LandmarkFpocket[pdbid] = np.array([df['centroid_id'].sum() , df.shape[0], np.around(df['x_coord'].sum(),3), np.around(df['y_coord'].sum(),3), np.around(df['z_coord'].sum(),3)])\n",
    "\n",
    "import pickle\n",
    "with open(\"../Database-PDB/CHECKSUM_LandmarkFpocketIntegrity.pkl\", \"wb\") as fn:\n",
    "    pickle.dump(Integrity_LandmarkFpocket, fn,protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checksum Cleansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12563/12563 [00:00<00:00, 123044.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "PdbidList = sorted(glob.glob(\"../Database-PDB/cleansed/*.pdb\")) \n",
    "PdbidList = sorted(set([i.split(\"/\")[-1].split(\".\")[0] for i in PdbidList]))\n",
    "Integrity_Cleansed = {}\n",
    "for pdbid_i in tqdm.tqdm(range(len(PdbidList))):\n",
    "    pdbid = PdbidList[pdbid_i]\n",
    "    Integrity_Cleansed[pdbid] = os.path.getsize(\"../Database-PDB/cleansed/%s.pdb\" %(pdbid)) \n",
    "import pickle\n",
    "with open(\"../Database-PDB/CHECKSUM_CleansedIntegrity.pkl\", \"wb\") as fn:\n",
    "    pickle.dump(Integrity_Cleansed, fn,protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checksum Apo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12563/12563 [00:00<00:00, 120620.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "PdbidList = sorted(glob.glob(\"../Database-PDB/apo/*.pdb\")) \n",
    "PdbidList = sorted(set([i.split(\"/\")[-1].split(\".\")[0] for i in PdbidList]))\n",
    "Integrity_Apo = {}\n",
    "for pdbid_i in tqdm.tqdm(range(len(PdbidList))):\n",
    "    pdbid = PdbidList[pdbid_i]\n",
    "    Integrity_Apo[pdbid] = os.path.getsize(\"../Database-PDB/apo/%s.pdb\" %(pdbid)) \n",
    "import pickle\n",
    "with open(\"../Database-PDB/CHECKSUM_ApoIntegrity.pkl\", \"wb\") as fn:\n",
    "    pickle.dump(Integrity_Apo, fn,protocol=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483da45aec43309c24cf49001fd3be9352115ff3f8b01e51a9517882487d0ea9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Nucl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
